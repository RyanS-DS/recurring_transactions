{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Recurring Payments in Bank Statements\n",
    "\n",
    "## Info\n",
    "\n",
    "- See final cell for complete solution\n",
    "- To install all required packages to your environment:\n",
    "`$ pip install -r requirements.txt`\n",
    "\n",
    "## Prior Reading\n",
    "\n",
    "- Financial Forecasting and Analysis for Low-Wage Workers: https://arxiv.org/pdf/1806.05362.pdf\n",
    "    - Good information on extracting recurring transactions and forecasting balance\n",
    "    - However, extraction works on defined recurrence periods (monthly, semi-monthly, weekly, bi-weekly)\n",
    "    - I want the extraction to be period-agnostic, but still be able to return the period in days\n",
    "    \n",
    "    \n",
    "## Rough strategy\n",
    "I see the problem falling roughly into 2 stages:\n",
    "- Clustering transactions by payee/payer\n",
    "- Classifying clusters as recurring or non-recurring\n",
    "\n",
    "For clustering I only used the transaction description\n",
    "\n",
    "For classifying I used the number of days between payments\n",
    "\n",
    "This means I did not use the amount feature. I felt it wasn't suitable to be used in the clustering because I don't think we want a change in payment amount to indicate a different series of payments. For example with Spotify, if you upgraded to the family plan for £14.99, I wouldn't want those transactions to be in a different cluster than previous payments of £9.99. I also didn't use it for classifying payments as recurring, due to amount not being mentioned in the definition in the problem statement.\n",
    "> The definition of a regularly recurring transaction is any debit or credit transaction to or from the same entity that happens at regular intervals.\n",
    "\n",
    "\n",
    "## Data import and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('example_data.csv')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1210 rows, no NULL values at all (good!)\n",
    "\n",
    "First observation is that the made_on column is an object, not a datetime; fixing this will make our lives easier later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert made_on column to datetime\n",
    "df['made_on'] = pd.to_datetime(df['made_on'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection shows first column is just an index, and `user_id` is OpenBanking user_id. Neither are useful so will drop both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop first two columns of df\n",
    "df = df.drop(df.columns[:2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate df into credits and debits\n",
    "credit_df = df[df['income'] == True]\n",
    "debit_df = df[df['income'] == False]\n",
    "\n",
    "print(f'Number of credits = {len(credit_df)}')\n",
    "print(f'Number of debits = {len(debit_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big imbalance between the amount of credits and debits; in another case this may indicate using a different method for credits and debits could be useful.\n",
    "\n",
    "Going to make a slightly clearer column for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'] = df['income'].map(lambda x: 'credit' if x == True else 'debit')\n",
    "\n",
    "df[['type', 'income']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description cleaning\n",
    "\n",
    "Generally most of the performance when clustering text data will come from proper preprocessing.\n",
    "\n",
    "The typical transaction string contains useful information, but is messy.\n",
    "\n",
    "E.G. `TFL TRAVEL CH ON 03 APR CLP` Tells us that the transaction was for TFL Travel on the 3rd April and that the transaction was a Charge (rather than a Refund)\n",
    "\n",
    "Hopefully removing these common strings can help us cluster payments better, since we do not want to cluster all Direct Debits or Fund Transfers together.\n",
    "\n",
    "`DDR` or `D/D` also appears in transactions, meaning direct debit. This may prove useful as a feature in future, however was not necessary in the method I ended up with, so I removed those codes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Big Regex pattern to remove undesirable parts of the description\n",
    "# \\b begins and ends the pattern\n",
    "# | indicates OR\n",
    "# \\d{2} - 2-digit numbers - commonly used for day in date\n",
    "# JAN-DEC - commonly used for month in date\n",
    "# ON|BCC|CLP|ATM|CH|FT|BGC - common abbreviations that indicate the type of transaction\n",
    "\n",
    "date_pattern = r'\\b(\\d{2}|JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC|ON|BCC|CLP|ATM|CH|FT|BGC|DDR|\\')\\b'\n",
    "df['clean_desc'] = df['original_description'].str.replace(date_pattern, '')\n",
    "\n",
    "#replace '*', '.', or ',' with a space\n",
    "df['clean_desc'] = df['clean_desc'].str.replace(r'\\*|\\.|\\,',' ', regex = True)\n",
    "\n",
    "#convert to lower case\n",
    "df['clean_desc'] = df['clean_desc'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some dates and messy bits in there E.G.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[31,'clean_desc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but working on an 80-20 basis this feels good enough. Descriptions are now much cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['original_description', 'clean_desc']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, I had a go at vectorising the description string using TF-IDF, and clustering with K-Means. However, 3 issues came with this:\n",
    "- Sample size isn't really big enough for good accuracy\n",
    "- No ideal value for K without manually checking the transactions, which is ungeneralisable\n",
    "- I don't really have enough time to model this properly!\n",
    "\n",
    "Instead I ended up doing something quite primitive, but effective; taking the first word of each transaction's cleaned description and combining with either credit or debit type (to make sure we don't combine recurring credits and debits to the same payer/payee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take just the first word (splitting by space) and concatenate with a space and credit/debit\n",
    "df['keyword'] = df['clean_desc'].map(lambda x: x.split(' ', 1)[0]) + ' ' + df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['keyword'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with 278 unique transaction descriptions to start classifying as recurring or non-recurring.\n",
    "\n",
    "This isn't completely fool-proof however! There are 4 transactions where the original description are websites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['keyword'] == 'www debit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These have been grouped into 1 `www debit` group, which isn't ideal, but, again, 80-20.\n",
    "\n",
    "(On retrospect, the PureGym one is the first payment in a recurring series, it would have been nice to capture this one too.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding recurring payments\n",
    "\n",
    "We're generally going to find recurring payments by looking at the period between payments.\n",
    "\n",
    "In order to do that I first need to calculate the period for each cluster of transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daydelta(df, date_col):\n",
    "    #Param df: dataframe containing the date column\n",
    "    #Param date_col: string of the name of the date column to be delta'd\n",
    "    #Returns dataframe with daydelta column appended\n",
    "    #Sort dataframe by date column\n",
    "    process_df = df.sort_values(date_col)\n",
    "    #Make new column that is the date_col minus the date_column in the previous row\n",
    "    process_df['daydelta'] = process_df[date_col] - process_df[date_col].shift()\n",
    "    return process_df\n",
    "\n",
    "def get_subdf(df, keyword):\n",
    "    #Small function to find just the rows containing each keyword, ordered by date\n",
    "    #Param df: dataframe to be searched\n",
    "    #Param keyword: keyword to be looked for\n",
    "    sub_df = df[df['keyword'] == keyword]\n",
    "    #Sort values by date\n",
    "    sub_df = sub_df.sort_values('made_on')\n",
    "    return sub_df\n",
    "\n",
    "#Add the daydelta column to the dataframe containing keyword 'spotify debit'\n",
    "daydelta(get_subdf(df, 'spotify debit'), 'made_on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to do it for all clusters within our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the daydeltas for each cluster:\n",
    "#For each unique keyword\n",
    "for i in df['keyword'].unique():\n",
    "    #Get just the dataframe with that keyword\n",
    "    sub_df = get_subdf(df, i)\n",
    "    #Create the daydelta column for sub_df\n",
    "    sub_df = daydelta(sub_df, 'made_on')\n",
    "    #For each row in this sub_df\n",
    "    for index, row in sub_df.iterrows():\n",
    "        #Assign the daydelta value to the original dataframe\n",
    "        df.loc[index, 'daydelta'] = sub_df.loc[index, 'daydelta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 1 NaT value for each unique keyword (of which we have 278), therefore there are (1210-278) = 932 non-null rows in the day delta column.\n",
    "\n",
    "Converting it to a proper Pandas Timedelta column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting daydelta column to timedelta \n",
    "df['daydelta'] = pd.to_timedelta(df['daydelta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And making a daydelta_int column that is the daydelta as an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the number of days in the timedelta\n",
    "df['daydelta_int'] = df['daydelta'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to classify them into recurring/non-recurring.\n",
    "\n",
    "\n",
    "\n",
    "After a bit of experimentation with z-scores etc, I came up with a fairly simple rule for classifying recurring payments:\n",
    "\n",
    "If the 3 most recent payments are within +-20% of their mean, it is recurring, otherwise it's non-recurring.\n",
    "\n",
    "Let's see which payments pass this rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_recurring(df, tolerance):\n",
    "    #Function that checks if the last 3 daydeltas are within given tolerance\n",
    "    #get 3 last daydeltas\n",
    "    recent_trans = df['daydelta_int'][-3:]\n",
    "    #calculate the mean of these daydeltas\n",
    "    avg_amount = recent_trans.mean()\n",
    "    #Define upper and lower bound given tolerance\n",
    "    lower_bound = avg_amount*(1-tolerance)\n",
    "    upper_bound = avg_amount*(1+tolerance)\n",
    "    #check if all daydeltas are between lower and upper bound\n",
    "    output = all(lower_bound <= x <= upper_bound for x in recent_trans)\n",
    "    return output\n",
    "\n",
    "#Printing keywords that pass our rule\n",
    "#For each unique keyword\n",
    "for keyword in df['keyword'].unique():\n",
    "    #Get just the dataframe with that keyword\n",
    "    sub_df = get_subdf(df, keyword)\n",
    "    #If it passses our test with 20% tolerance\n",
    "    if check_recurring(sub_df, 0.20) == True:\n",
    "        #Print the keyword\n",
    "        print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look like pretty sensible suggestions for recurring payments\n",
    "\n",
    "Now to mark the entire df as True or False for recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each unique keyword\n",
    "for keyword in df['keyword'].unique():\n",
    "    #Get the dataframe with that keyword\n",
    "    sub_df = get_subdf(df, keyword)\n",
    "    #If it passses our test with 20% tolerance\n",
    "    if check_recurring(sub_df, 0.2) == True:\n",
    "        #For the indexes within that sub_df, assign True to the 'recurring' column in the original dataframe\n",
    "        df.loc[sub_df.index, 'recurring'] = True\n",
    "#Assign false to all other rows\n",
    "df['recurring'].fillna(False, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our full dataframe with all transactions marked as recurring or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Entity Name\n",
    "\n",
    "To make a nice looking entity name, I will just use any words which appear in all occurences of the cleaned description. This is good because it should pick out the company name, but avoid dates, payment reference numbers etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_entity_name(data):\n",
    "    #Param data: should be a column of a data frame containing the cleaned descriptions\n",
    "    #Returns a string of words which appear in all columns\n",
    "    common_words = ''\n",
    "    #Turn column of strings into list of strings\n",
    "    strings = list(data)\n",
    "    #For each word in the first description\n",
    "    for word in strings[0].split():\n",
    "        #If that word is in all descriptions\n",
    "        if all(word in desc for desc in strings):\n",
    "            #Concat that word and a space to the common_words string\n",
    "            common_words += word + ' '\n",
    "    #Remove final space\n",
    "    common_words = common_words.strip()\n",
    "    return common_words\n",
    "\n",
    "#Find the entity name for the dropbox recurring payments\n",
    "find_entity_name(get_subdf(df, 'dropbox debit')['clean_desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the other needed features - income, period_days, and typical_amount_cents.\n",
    "\n",
    "`income`: Get the first value of the 'income' column in each sub_df. Since we categorised by credit/debit at the clustering stage, all values for income in each sub_df will be the same.\n",
    "\n",
    "`period_days`: Average the last 3 daydeltas\n",
    "\n",
    "`typical_amount_cents`: Average all payment amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurring_keywords(df):\n",
    "    #Small function to get a list of keywords we have defined as recurring\n",
    "    keywords = df[df['recurring'] == True]['keyword'].unique()\n",
    "    return list(keywords)\n",
    "\n",
    "def generate_output(df):\n",
    "    #Get the recurring keywords\n",
    "    recurring_keywords = get_recurring_keywords(df)\n",
    "    #Create empty list\n",
    "    output = []\n",
    "    #For each recurring keyword\n",
    "    for keyword in recurring_keywords:\n",
    "        #Create blank dictionary\n",
    "        info_dict = {}\n",
    "        #Get the dataframe with that keyword\n",
    "        sub_df = get_subdf(df, keyword)\n",
    "        #Assign features to the dictionary\n",
    "        info_dict['entity_name'] = find_entity_name(sub_df['clean_desc'])\n",
    "        info_dict['income'] = sub_df['income'].iloc[0]\n",
    "        info_dict['period_days'] = sub_df['daydelta_int'][-3:].mean().round(0)\n",
    "        info_dict['typical_amount_cents'] = sub_df['amount_cents'].mean().round(0)\n",
    "        #Append the dictionary to the output list\n",
    "        output.append(info_dict)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning summary of recurring transactions\n",
    "\n",
    "The below combines all of the above code into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS TO GET FINAL OUTPUT\n",
    "\n",
    "#UTILITY FUNCTIONS\n",
    "\n",
    "def get_subdf(df, keyword):\n",
    "    #Small function to find just the rows containing each keyword, ordered by date\n",
    "    #Param df: dataframe to be searched\n",
    "    #Param keyword: keyword to be looked for\n",
    "    sub_df = df[df['keyword'] == keyword]\n",
    "    #Sort values by date\n",
    "    sub_df = sub_df.sort_values('made_on')\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "#MAIN FUNCTIONS\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    #Convert made_on column to datetime\n",
    "    df['made_on'] = pd.to_datetime(df['made_on'])\n",
    "    #drop first two columns of df\n",
    "    df = df.drop(df.columns[:2], axis = 1)\n",
    "    #create credit/debit type column\n",
    "    df['type'] = df['income'].map(lambda x: 'credit' if x == True else 'debit')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def clean_description(df):\n",
    "    #Big Regex pattern to remove undesirable parts of the description\n",
    "    # \\b begins and ends the pattern\n",
    "    # | indicates OR\n",
    "    # \\d{2} - 2-digit numbers - commonly used for day in date\n",
    "    # JAN-DEC - commonly used for month in date\n",
    "    # ON|BCC|CLP|ATM|CH|FT|BGC - common abbreviations that indicate the type of transaction\n",
    "\n",
    "    date_pattern = r'\\b(\\d{2}|JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC|ON|BCC|CLP|ATM|CH|FT|BGC|DDR|\\')\\b'\n",
    "\n",
    "    df['clean_desc'] = df['original_description'].str.replace(date_pattern, '')\n",
    "\n",
    "    #replace '*', '.', or ',' with a space\n",
    "    df['clean_desc'] = df['clean_desc'].str.replace(r'\\*|\\.|\\,',' ', regex = True)\n",
    "\n",
    "    #convert to lower case\n",
    "    df['clean_desc'] = df['clean_desc'].str.lower()\n",
    "    \n",
    "    #create keyword column containing first word of cleaned description and credit/debit\n",
    "    df['keyword'] = df['clean_desc'].map(lambda x: x.split(' ', 1)[0]) + ' ' + df['type']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def daydelta(df, date_col):\n",
    "    #Param df: dataframe containing the date column\n",
    "    #Param date_col: string of the name of the date column to be delta'd\n",
    "    #Returns dataframe with daydelta column appended\n",
    "    #Sort dataframe by date column\n",
    "    process_df = df.sort_values(date_col)\n",
    "    #Make new column that is the date_col minus the date_column in the previous row\n",
    "    process_df['daydelta'] = process_df[date_col] - process_df[date_col].shift()\n",
    "    return process_df\n",
    "\n",
    "def calculate_daydelta(df):\n",
    "    for i in df['keyword'].unique():\n",
    "        sub_df = get_subdf(df, i)\n",
    "        sub_df = daydelta(sub_df, 'made_on')\n",
    "        for index, row in sub_df.iterrows():\n",
    "            df.loc[index, 'daydelta'] = sub_df.loc[index, 'daydelta']\n",
    "    \n",
    "    df['daydelta'] = pd.to_timedelta(df['daydelta'])\n",
    "    df['daydelta_int'] = df['daydelta'].dt.days\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_recurring(df, tolerance):\n",
    "    recent_trans = df['daydelta_int'][-3:]\n",
    "    avg_amount = recent_trans.mean()\n",
    "    lower_bound = avg_amount*(1-tolerance)\n",
    "    upper_bound = avg_amount*(1+tolerance)\n",
    "    output = all(lower_bound <= x <= upper_bound for x in recent_trans)\n",
    "    return output\n",
    "\n",
    "def calculate_recurring(df):\n",
    "    for i in df['keyword'].unique():\n",
    "        sub_df = get_subdf(df, i)\n",
    "        if is_recurring(sub_df, 0.2) == True:\n",
    "            df.loc[sub_df.index, 'recurring'] = True\n",
    "    df['recurring'].fillna(False, inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_entity_name(data):\n",
    "    #Param data: should be a column of a data frame containing the cleaned descriptions\n",
    "    #Returns a string of words which appear in all columns\n",
    "    common_words = ''\n",
    "    #Turn column of strings into list of strings\n",
    "    strings = list(data)\n",
    "    #For each word in the first description\n",
    "    for word in strings[0].split():\n",
    "        #If that word is in all descriptions\n",
    "        if all(word in desc for desc in strings):\n",
    "            #Concat that word and a space to the common_words string\n",
    "            common_words += word + ' '\n",
    "    #Remove final space\n",
    "    common_words = common_words.strip()\n",
    "    return common_words\n",
    "\n",
    "def get_recurring_keywords(df):\n",
    "    #Small function to get a list of keywords we have defined as recurring\n",
    "    keywords = df[df['recurring'] == True]['keyword'].unique()\n",
    "    return list(keywords)\n",
    "\n",
    "def generate_output(df):\n",
    "    #Get the recurring keywords\n",
    "    recurring_keywords = get_recurring_keywords(df)\n",
    "    #Create empty list\n",
    "    output = []\n",
    "    #For each recurring keyword\n",
    "    for keyword in recurring_keywords:\n",
    "        #Create blank dictionary\n",
    "        info_dict = {}\n",
    "        #Get the dataframe with that keyword\n",
    "        sub_df = get_subdf(df, keyword)\n",
    "        #Assign features to the dictionary\n",
    "        info_dict['entity_name'] = find_entity_name(sub_df['clean_desc'])\n",
    "        info_dict['income'] = sub_df['income'].iloc[0]\n",
    "        info_dict['period_days'] = sub_df['daydelta_int'][-3:].mean().round(0)\n",
    "        info_dict['typical_amount_cents'] = sub_df['amount_cents'].mean().round(0)\n",
    "        #Append the dictionary to the output list\n",
    "        output.append(info_dict)\n",
    "    return output\n",
    "\n",
    "\n",
    "#DRIVER FUNCTION\n",
    "    \n",
    "def get_recurring_transactions(df):\n",
    "    df = clean_dataframe(df)\n",
    "    df = clean_description(df)\n",
    "    df = calculate_daydelta(df)\n",
    "    df = calculate_recurring(df)\n",
    "    output = generate_output(df)\n",
    "    return output\n",
    "\n",
    "transactions = pd.read_csv('example_data.csv')\n",
    "recurring = get_recurring_transactions(transactions)\n",
    "recurring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
